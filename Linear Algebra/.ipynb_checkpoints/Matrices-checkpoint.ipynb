{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628377cb",
   "metadata": {},
   "source": [
    "# Matrices\n",
    "## Enter the matrix\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a_{11} &  a_{12}  & \\ldots & a_{1n}\\\\\n",
    "a_{21}  &  a_{22} & \\ldots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n1}  &   a_{n2}       &\\ldots & a_{nn}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Basics\n",
    "\n",
    "Matrices are a set of very handy notations in Mathematics and are a core concept in Linear Algebra. Matrices can be viewed in many different ways, just a spreadsheet of numbers, columns of vectors, or the transformations, where every column represent where every unit vector will end up after applying the transformation to a vector (dot product). For example, in 2x2 matrix, the 2 columns can be viewed as the coordinates where the unit vectors $\\hat{i}$ (X-axis) and unit vector $\\hat{j}$ (Y-axis) ends up (from (1, 0) & (0, 1). \n",
    "\n",
    "**Note that to be able to apply a matrix to another entity, the number of rows of the matrix needs to equal the number of columns of the target**\n",
    "\n",
    "### Linear Transformations\n",
    "Lets define a matrix and the unit vectors in 2 dimensions:\n",
    "\n",
    "$A =\\begin{bmatrix}\n",
    "    2 & 4 \\\\\n",
    "    3 & 7 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{i} =\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{j} =\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We want to apply the matrix transformation to the vector \n",
    "\n",
    "$v =\\begin{bmatrix}\n",
    "    5\\\\\n",
    "    4\n",
    "\\end{bmatrix}$\n",
    "\n",
    "which can be viewed as \n",
    "\n",
    "$v =\\begin{bmatrix}\n",
    "    5\\,\\hat{i}\\\\\n",
    "    4\\,\\hat{j}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "If the columns of A represents where the unit vectors will land after transformation,\n",
    "we can simply write down the following linear combination:\n",
    "\n",
    "$5\\begin{bmatrix}\n",
    "    2\\\\\n",
    "    3\n",
    "\\end{bmatrix} + 4\\begin{bmatrix}\n",
    "    4\\\\\n",
    "    7\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    26\\\\\n",
    "    43\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Which is the same as taking the dot product like:\n",
    "\n",
    "$A\\cdot v = \\begin{bmatrix}\n",
    "    2 & 4 \\\\\n",
    "    3 & 7 \\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "    5\\\\\n",
    "    4\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    26\\\\\n",
    "    43\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Matrix inversion $Ax = b, x = A^{-1}b $\n",
    "The inversion of a matrix is basically what matrix times the result $b$ of the first matrix transformation (linear combination) $Ax$ equals to $x$, i.e how to find the vector x if you have matrix A and the result b. \n",
    "\n",
    "Not all matrices are invertible, but when they are, finding the inversion of a matrix is usually done with Gaussian Elimination. \n",
    "\n",
    "Important aspect regarding matrix inversion and how to find them and WHY we can find them with Gaussian Elimination, let's say we have a combined matrix of $A$ and $I$ like:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    A & I\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "By Gauss-Jordan we eliminate untill we turn A into I. This can be done with elimination matrix $E$ which contains all the steps of elimination, by multiplying the combined matrix\n",
    "\n",
    "$\n",
    "E\\begin{bmatrix}\n",
    "    A & I\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    I & E\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now, what is $E$?\n",
    "\n",
    "$\n",
    "\\frac{A}{A} = EA =\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{1}{A} \\times A = A^{-1}A = EA\n",
    "$\n",
    "\n",
    "$\n",
    "A^{-1} = E\n",
    "$\n",
    "\n",
    "The matrix that turns $A$ into $I$ is $E$ which is the inverse of $A$, $A^{-1}$.\n",
    "A much quicker proof is simply to divide the whole combined matrix with A directly to see the proof (See below for the next proof). \n",
    "\n",
    "$\\frac{\\begin{bmatrix}\n",
    "    A & I\n",
    "\\end{bmatrix}}{A}\n",
    "$\n",
    "\n",
    "A simple proof for $(A^{-1})^{-1} = A$\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\n",
    "\\begin{bmatrix}\n",
    "    A & I\n",
    "\\end{bmatrix}}{A} = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{A}{A} & \\frac{I}{A}\n",
    "\\end{bmatrix} =\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\n",
    "\\begin{bmatrix}\n",
    "    I & A^-1\n",
    "\\end{bmatrix}}{A^-1} = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{I}{\\frac{I}{A}} & \\frac{A^-1}{A^-1}\n",
    "\\end{bmatrix} =\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    A & I\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "#### Singular Matrices\n",
    "A singular matrix does **not** have an inverse. This happens when either the columns or the rows are multiples of each other or if combinations of them equals one of the other. Also, if you can find a vector that multiplies the matrix and produces $0$, it's singular. \n",
    "\n",
    "Geometrically if we look at the column picture described below, it means that the vectors points in the **same** direction and therefore they are the same vector by different scalars, hence singular, which even describes the word **singular** well.\n",
    "\n",
    "### Matrix Perspectives\n",
    "\n",
    "#### Row Picture\n",
    "The row picture is basically what we learn from the normal Algebra class, where we view every row of the matrix as the equation, f.e in 2 dimensions, x + y = 3 and draw the line that corresponds to that and the next row. The solution will be were those objects meet.\n",
    "\n",
    "#### Column Picture\n",
    "The column picture on the other hand is when we view the matrix as the vectors every column represents and draw the solution as a **linear combination**.\n",
    "\n",
    "\n",
    "\n",
    "### Matrix Multiplication\n",
    "If you multiply matrix $A$ with matrix $B$, the columns of $A$ needs to be the same amount as the rows of $B$. If we on the other hand multiply $B$ with $A$ the inverse is true, therefore, applying matrices in different orders matter.\n",
    "\n",
    "Multiplying $A$ and $B$ gives two perspecitves of pictures, $A$ times every column of $B$ or every row of $A$ times matrix $B$.\n",
    "\n",
    "Another way of multiplying two matrices is basically by multiplying **columns of $A$** with **rows of $B$** and then add the resulting matrices.\n",
    "\n",
    "#### Resulting Vector\n",
    "A matrix with m rows and n columns $m\\times n$ times a matrix with n rows times p columns $n \\times p$ will produce a matrix with m rows and p columns $m \\times p$. This is pretty self-explanatory when understanding that multiplying a matrix with another matrix is basically multiplying a matrix times p vectors (with n rows). A matrix times a vector produces a vector, it transforms the vector. \n",
    "\n",
    "#### Entries\n",
    "A specific entry in a matrix $e_{ij}$ after multiplication can be found by looking at how the multiplication acts. If we want to find $C_{11}$ after multiplying matrices $A$ and $B$, we simply take the dot product of row 1 in matrix $A$ and column 1 in matrix $B$\n",
    "\n",
    "\n",
    "#### In Action\n",
    "*Note that multiplying from left creates row vectors and multiplying from right creates column vectors*\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    e & f\\\\\n",
    "    g & h\n",
    "\\end{bmatrix}$\n",
    "\n",
    "When multiplying from left to right, you will multiply the rows of the right matrix by every instance of the left matrix and add the row vectors together. \n",
    "\n",
    "$a\\begin{bmatrix} e & f \\end{bmatrix} + b \\begin{bmatrix} g & h \\end{bmatrix} = \\begin{bmatrix} ae + bg & af + bh \\end{bmatrix}$\n",
    "\n",
    "$c \\begin{bmatrix}e & f \\end{bmatrix} + d \\begin{bmatrix} g & h \\end{bmatrix} = \\begin{bmatrix} ce + dg & cf + dh \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} ae + bg & af + bh\\\\\n",
    " ce + dg & cf + dh \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "When multiplying from right to left, the inverse is true, you will multiply the columns of the left matrix with the instances of the right. In my opinion, this one is easier too see. One row iteration to end per column on the right matrix creates one vector, then just put the created vectors in the same order!\n",
    "\n",
    "$e \\begin{bmatrix} a \\\\ c \\end{bmatrix} + g \\begin{bmatrix} b \\\\ d \\end{bmatrix}, \n",
    "f \\begin{bmatrix} a \\\\ c \\end{bmatrix} + h \\begin{bmatrix} b \\\\ d \\end{bmatrix} \n",
    "$\n",
    "\n",
    "$\\begin{bmatrix} ae + bg & af + bh\\\\\n",
    " ce + dg & cf + dh \\end{bmatrix}\n",
    "$\n",
    "\n",
    "As you can clearly see now, if we for some reason changed the order of the matrices, we still need to follow the same rules and hence the produced matrix will be different.\n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "    e & f\\\\\n",
    "    g & h\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$a \\begin{bmatrix} e \\\\ g \\end{bmatrix} + c \\begin{bmatrix} f \\\\ h \\end{bmatrix}, \n",
    "b \\begin{bmatrix} e \\\\ g \\end{bmatrix} + d \\begin{bmatrix} f \\\\ h \\end{bmatrix} \n",
    "$\n",
    "\n",
    "$\\begin{bmatrix} ae + cf & be + df \\\\\n",
    " ag + ch & bg + dh \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Elimination Matrix\n",
    "An elimination matrix is basically a matrix that can perform the steps of Gaussian Elimination just by multiplying. The notation could be f.e $E_{21}$ which would signify, the matrix that eleminates row 2 column 1 from the target, by definition turns it into 0. Worth to mention, in a singular case when the matrix is not invertible, elimination will product zeros in a row if that row is a combination of the others. \n",
    "\n",
    "Let's use an example, we have the matrix $A = \\begin{bmatrix}\n",
    "    2 & 1 & 1\\\\\n",
    "    2 & 2 & 0\\\\\n",
    "    0 & 1 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We can build the full elimination matrix $E$ by doing the steps in order $E_{nm}$.\n",
    "First we need to execute row 2 column 1 with $E_{21}$, then $E_{32}$ to get all the pivots.  \n",
    "\n",
    "$U = EA = E_{32}(E_{21}A) = (E_{32}E_{21})A$\n",
    "\n",
    "### $A = LU$ \n",
    "Matrix $A$ can also be factored to $LU$, where $U$ is the upper triangle of $A$ and $L$ is the lower triangle multiplying $U$ to create $A$. $L$ is basically acting on $U$'s rows to achieve this and stores the inverse of the information for the elimination from $A$ to $U$. $L$ is $E^{-1}$. We can also substitute $A = LU$ in the following way:\n",
    "\n",
    "$Ax = b$\n",
    "\n",
    "$A = LU$\n",
    "\n",
    "$LUx = b$\n",
    "\n",
    "$Lc = b$\n",
    "\n",
    "$c = L^{-1}b$\n",
    "\n",
    "$Ux = L^{-1}b$\n",
    "\n",
    "$Ux = c$\n",
    "\n",
    "$x = U^{-1}c$\n",
    "\n",
    "$x = U^{-1}L^{-1}b$\n",
    "\n",
    "$x = A^{-1}b$\n",
    "\n",
    "### Transposition \n",
    "Transposing matrices basically mean that the rows become columns and the columns become rows. \n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "\\end{bmatrix} $ \n",
    "\n",
    "$A^{T} = \\begin{bmatrix}\n",
    "    a & c\\\\\n",
    "    b & d\n",
    "\\end{bmatrix}$\n",
    "\n",
    "A multiplication of a matrix that gets transposed will also change the order of mulitplication. f.e \n",
    "\n",
    "$(AB)^{T} = B^{T}A^{T}$\n",
    "\n",
    "This reason for this is pretty intuitive, while there exist a more formla proof for it,\n",
    "for this it isn't now really necessary. As we know, if we multiply from left, $A$ acts on the rows of $B$ and if we multiply from right, $B$ acts on the columns of $A$. If we transpose both of them, of course the order of multiplication will also inverse to stay the same! If not, after transposition all of a sudden, $A$ would act on the columns of $B$, which of course would yield a different result for most matrices. If $B^T$ acts on the rows of $A^T$ it will of course be the same as if $A$ acts on the rows of $B$ and vice verse  \n",
    "\n",
    "#### Inner Product\n",
    "Can be explained with examples of two vectors, $x$ and $y$. The inner product is basically $x^{T}y$ which is the same as doing $x \\cdot y$ and produces a number. Thinking of vector multiplication or dot product in general in this terms makes more sense in many cases. This produces a scalar/number as expected. The outer product on the other hand is when a column vector is multiplying a row vector, $xy^{T}$ this is interesting, because it will produce a matrix, m x p.\n",
    "\n",
    "#### Symmetric Matrices\n",
    "A matrix where $S^{T} = S$ is symmetric, the columns equals the rows. One interesting thing to consider is that a matrix times its transpose is always symmetric: $A^{T}A$:\n",
    "\n",
    "$A^{T}A = A^T({A^T})^T = A^{T}A $\n",
    "\n",
    "Important to consider, the order of multiplication can produce different matrices but both of them will still be symmetric even though they have different shapes!\n",
    "\n",
    "#### Permutation Matrices\n",
    "The purpose of these square matrices is to swap rows of other matrices. There exists only $n!$ different matrices where n i number of columns (and rows). The inverse $P^{-1}$ is always $P^{T}$\n",
    "\n",
    "Important consideration of $A = LU$, in reality for all matrices it is actually $PA = LU$ as we need to apply row exchanges to some matrices before elemination towards $U$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b039c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,3],[2,7]])\n",
    "B = np.array([[7,-3],[-2,1]])\n",
    "\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b4a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f12e77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
