{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ddd56d",
   "metadata": {},
   "source": [
    "# Orthogonality\n",
    "## The 90 degrees\n",
    "\n",
    "So orthogonal is another word for perpendicular and simply means that the angle difference between them are 90 degrees. This is important for many aspects, and can also be used to prove pythagoras theorem. \n",
    "\n",
    "### Finding the plane of the linear combinations\n",
    "Finding the plane of the linear combinations of f.e 2 vectors in $\\mathbb{R}^3$ can be done by thinking orthogonal. \n",
    "Let the linear combinations of $cv + dw = L$ and the resulting plane $P$ needs to go through the zero vector, and resides in 3 dimensions. This means that the plane will be some combination of x's, y's and z's that equals 0. The reason for that is simple, if the vectors go from origin, which the should in linear algebra and you choose variables that are all 0, we still need to end up at zero. \n",
    "\n",
    "This means that we need to find some combination $e$ of the components of $L$ and that combination needs to produce $0$ in order to satisfy the conditions for the plane. The combination $e$ holds the coefficients for that plane. The reason for this is pretty simple, we have a vector with 3 components $L$, $(c_1, c_2, c_3)$ and those equals $(x, y, z)$ respectively. A combination of the components in $(x, y, z) = (c_1, c_2, c_3)$ always need to produce 0, which means that every point or vector is only part of the plane if their combination equals 0 when applied with the equation. That equation produces a plane, which can be any plane in this case, in $\\mathbb{R}^3$ as long as it goes through $(0, 0, 0)$. \n",
    "\n",
    "So what does this have to do with orthogonality? Well, some combination $e$ which can act as an elimination vector, multiplies that vector $L$ that holds all the linear combinations, in this case, two independent vectors. The combination $e$ needs to be orthogonal to $L$ in order to satisfy the equation to the plane, i.e when $eL = 0$ \n",
    "\n",
    "Let's say we have two independent vectors that creates a 2 dimensional plane in $\\mathbb{R}^3$:\n",
    "\n",
    "$v = c \\begin{bmatrix}\n",
    "    3\\\\\n",
    "    5\\\\\n",
    "    2 \\end{bmatrix}\\;\n",
    "w = d \\begin{bmatrix}\n",
    "    1\\\\\n",
    "    2\\\\\n",
    "    -1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "All linear combinations:\n",
    "\n",
    "$L = \\begin{bmatrix}\n",
    "    3c + d\\\\\n",
    "    5c + 2d\\\\\n",
    "    2c -d\n",
    "    \\end{bmatrix}   \n",
    "$\n",
    "\n",
    "Some combinations $e$ of the rows/components in this case, produces zero, regardless of $c$ and $d$. The dot product between $eL = 0$ and they are perpendicular. An interesting fact here is that, if we think of the nullspace for a second to a matrix $A$, the nullspace is perpendicular to the row space because all the vectors in $N(A)$ turns the rows of $A$ into 0. When finding the nullspace we simply ask, what combination of the rows of $A$ gives us 0? Of course all the solutions to that need to be orthogonal. This is the same with the columns of a matrix, which we can imagine our above vectors as, the columns of $A$. Any vector from the left nullspace, in this case $e$ will satisfy our answer and therefore $e$ will give us the coefficients to the plane. Therefore the most systemetic way of finding the plane, is simply to put the vectors back to a matrix $A$ and solve for the left nullspace. \n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "    3 & 1\\\\\n",
    "    5 & 2\\\\\n",
    "    2 & -1\n",
    "    \\end{bmatrix}\n",
    "    \\; A^T = \\begin{bmatrix}\n",
    "    3 & 5 & 2\\\\\n",
    "    1 & 2 & -1\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "As column 1 and 2 are independent to each other, we know that the third column is a combination of those, because this matrix can't have a rank which is more than 2, which we also know from the fact that we picked two independent vectors and transposed them. To be really clear, we can reduce this matrix to $R$ as much as possible as that won't change the column space of $A$ nor the left nullspace. The reason for this is because now with $A^T$ the elimination process will create combinations of the columns which stays in the column space. The left nullspace is also intact, because if the column space is intact, the same vectors in the left nullspace will still be orthogonal because the whole subspaces are orthogonal to each other. \n",
    "\n",
    "After some elimination and row exchanges, we reach:\n",
    "$R =\\begin{bmatrix}\n",
    "    1 & 0 & 9\\\\\n",
    "    0 & 1 & -5\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Solve for the special solutions in $N(A^T)$ by standard procedure and we end up with:\n",
    "\n",
    "$\n",
    "e = k\\begin{bmatrix}\n",
    "    -9\\\\\n",
    "    5\\\\\n",
    "    1\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$e(x, y, z)$ is our plane! If you noticed, $k$ is any constant in this case. \n",
    "\n",
    "To double check, let's check $eL$ and hopefully it is 0:\n",
    "\n",
    "$ \\begin{bmatrix}\n",
    "    -9\\\\\n",
    "    5\\\\\n",
    "    1\n",
    "    \\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "    3c + d\\\\\n",
    "    5c + 2d\\\\\n",
    "    2c -d\n",
    "    \\end{bmatrix}   \n",
    "$\n",
    "\n",
    "$-9(3c + d) + 5(5c + 2d) + 1(2c - d) = (-27c + 25c + 2c) + (10d - 9d - d) = 0$\n",
    "\n",
    "The equation of the plane is:\n",
    "$-9x + 5y + z = 0$\n",
    "\n",
    "and sure is, it's perfectly zero and orthogonal. And of course, to find the plane to the rows of $A$, the coefficients are in the nullspace $N(A)$, and obviously in this case with two independent row vectors in $\\mathbb{R^2}$ that will just fill the 2 dimensional space and the equation will only be $z = 0$ which will be a flat lying plane on the \"ground\". Image below shows $-9x + 5y + z = 0$ with our two vectors, beautifully aligned.  \n",
    "\n",
    "<img src=\"pngs/orthogonal-plane.png\" width=\"300\">\n",
    "\n",
    "If the nullspaces simply contain the zero vector, that means that the subspaces orthogonal to the nullspaces (row/column - space) span the whole space they reside in.\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix}\n",
    "    1 & 0\\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "What space does the row space span? Look at the nullspace, the nullspace only contains the 0 vector, so in this case it spans the full dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551dada",
   "metadata": {},
   "source": [
    "## Projection\n",
    "Orthogonality is very important for a specific problem called projection, when we want to project something unto another subspace. This is key if we want to \"solve\" an unsolveable problem, with an added error. The standard problem to solve with projection is the least square problem, fitting the best possible line to a number of vectors. Before solving that, let's check how projection itself works.\n",
    "\n",
    "### Formula & Proof\n",
    "Proving the projection formula is pretty straightforward and it works basically the same for vector and matrices except a notational difference. I'm gonna use the following image to explain projection and the exact vectors and matrices according to this image. Unfortunately I selected a plane that was a bit awkward so the projection matrix $P$ has many fractions, but not too bad!\n",
    "\n",
    "<img src=\"pngs/projection-example.png\" width=\"400\" >\n",
    "\n",
    "What's our goal here?\n",
    "We want to find the projection of $b$ on the plane $A$, through the vector $e$ which is the shortest path to $A$. This means that we want to solve $Ax = b$ with an added error amount and to get the minimal error, we choose to take the shortest path thorugh vector $e$. Notice that the shortest path is always a $90^{\\circ}$ angle to the target, according to the pythagorean theorem, and a very important point for this formula. The vector $e$ is always orthogonal to the subspace $A$. \n",
    "\n",
    "If we use some multiple of $A$ we will find the vector $p$, simply because some combinations of $A$'s vectors will result in $p$. The multiple we will call $\\hat{x}$. The reason for this is because $Ax = b$ is unsolveable, but not $A\\hat{x} = b$. Therefore the goal is to find $\\hat{x}$. As we see according to the picture, vector $e = b - p$. $p$ is the projected $b$ therefore $A\\hat{x} = p$ so we have the equation $e = b - A\\hat{x}$. We know that $e$ is orthogonal to the plane, therefore $Ae = 0$, which means that every vector in subspace A is orthogonal to $e$, which then of course is in $N(A^T)$.\n",
    "\n",
    "$A^{T}b - A^{T}A\\hat{x} = 0$\n",
    "\n",
    "$A^{T}b = A^{T}A\\hat{x} $\n",
    "\n",
    "$(A^{T}A)^{-1}A^{T}b = \\hat{x} $\n",
    "\n",
    "$A\\hat{x} = p$\n",
    "\n",
    "$A(A^{T}A)^{-1}A^{T}b = p$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbcafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
