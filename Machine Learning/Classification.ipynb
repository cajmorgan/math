{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d342855",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## A useful predictor\n",
    "Classification is a class of very useful Machine Learning Algorithms that has a **discrete** output, which means that the output is limited to a set of values. There are a lot of areas where classification really shines and with the help of these methods, it's possible to create human like decisions in a lot of different applications.\n",
    "\n",
    "There are a bunch of different classification algorithms such as **Logistic Regression**, **Random Forest**, **K-Nearest Neighbors**. Classification can be either binary (Yes/No) or multi-class, but in general, every classification algorithm can be either. \n",
    "\n",
    "## Algorithms\n",
    "\n",
    "### Logistic Regression\n",
    "The logistic regression model function $h_\\theta(x)$ can be defined with the sigmoid function as:\n",
    "\n",
    "## $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{Tx}}}$\n",
    "\n",
    "The output will be limited to $0 \\le h_\\theta(x) \\le 1$\n",
    "\n",
    "\n",
    "The output will be a probability score. \n",
    "\n",
    "#### Cost Function\n",
    "The cost to minimize in Logistic Regression can be defined as:\n",
    "\n",
    "### $ E(\\theta) = \\frac{1}{n}\\times\\displaystyle\\sum_{i=1} ^n L(h_\\theta(x_i) - y_i)$\n",
    "\n",
    "### $L(h_\\theta(x), y) = \\begin{cases}\n",
    "      -log_2(h_\\theta(x)) \\;\\;if y=1\\\\\n",
    "      -log_2(1 - h_\\theta(x)) \\;\\;if y=0\\\\\n",
    "    \\end{cases} $\n",
    "    \n",
    "To explain this in a simple way, when the true value $y=1$ and $h_\\theta(x)$ is closer to $1$, the cost will be closer to $0$, i.e lower cost and vice versa. This goes in the opposite direction when the true ouput i $y=0$ and $h_\\theta(x)$ is closer to $0$, the cost is closer to $0$ as well, which is dedicated by the piecewise functions if statements. \n",
    "\n",
    "\n",
    "#### Multi-class classification\n",
    "As the cost function suggests Logistic Regression only works for binary classification, that's not true. We can use it for multi-class outputs as well by a simple idea. If we treat the every instance as \"one-vs-all\", we can predict a probability for every possible class and choose the one that's highest. **There are much more details into this and I will update this section later**.\n",
    "\n",
    "\n",
    "\n",
    "### K-Nearest Neighbors\n",
    "Is probably the most simple algorithm in Machine Learning, regardless if you use it as classification or regression, but even if it's pretty dumb, it's very fast. So basically you set the value $K$ and when a new prediction happens, it iterates through all the points closest to the new predictor and picks the majority class of the $K$ closests neighbors.   \n",
    "\n",
    "\n",
    "\n",
    "### Random Forest / Decision Trees\n",
    "\n",
    "#### Decision Tree\n",
    "A decision tree is basically a Q&A algorithm where you can imagine it as a tree or if you are a programmer nested if-else statements. In practice, of course it's not a nested tree of if-elses statements, but in theory it could be implemented in that way, but a decision tree is fortunately way more general. \n",
    "\n",
    "A decision tree is created by testing the amount of **impurity** by each split of a region $R$ into two smaller regions $R_1$ & $R_2$. The region to split is chosen by testing the different features and tresholds to find the split with lowest impurity in both regions and then recursively continues until it meets a certain criteria, like there won't be any split that improves the impurity in the next region 2 regions from the previous one. \n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "Is a so called **Ensemble Method** which is a definition of a combination of multiple algorithms working together to produce a more accurate result. A Random Forest are basically $n$ numbers of decision trees built from one dataset, by in a general manner randomly picking different observations for each tree. After that process is done, it checks what the majority of the trees predicts on a new value and picks the **majority vote** as the predicted class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b2f2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Code example with Logistic Regression\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv(\"datasets/university-admission-dataset.csv\") \n",
    "\n",
    "#Extracting the independent and dependent variables\n",
    "X = df[[\"math\", \"english\"]].values\n",
    "y = df[\"admission\"].values\n",
    "\n",
    "#As there could be a very large difference between the independent features, we need to scale them.\n",
    "#There is a page available to understand this process in the directory.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "#Split the train data and test data so we can test the accuracy later\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Train the model to find the best optimum parameters for this dataset\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prediction_test = model.predict(X_test)\n",
    "\n",
    "accuracy_score(prediction_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710171ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
