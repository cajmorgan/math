{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d342855",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## A useful predictor\n",
    "Classification is a class of very useful Machine Learning Algorithms that has a **discrete** output, which means that the output is limited to a set of values. There are a lot of areas where classification really shines and with the help of these methods, it's possible to create human like decisions in a lot of different applications.\n",
    "\n",
    "There are a bunch of different classification algorithms such as **Logistic Regression**, **Random Forest**, **K-Nearest Neighbors**. Classification can be either binary (Yes/No) or multi-class, but in general, every classification algorithm can be either. \n",
    "\n",
    "## Algorithms\n",
    "\n",
    "### Logistic Regression\n",
    "The logistic regression model function $h_\\theta(x)$ can be defined with the sigmoid function as:\n",
    "\n",
    "## $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^{Tx}}}$\n",
    "\n",
    "The output will be limited to $0 \\le h_\\theta(x) \\le 1$\n",
    "\n",
    "\n",
    "The output will be a probability score. \n",
    "\n",
    "#### Cost Function\n",
    "The cost to minimize in Logistic Regression can be defined as:\n",
    "\n",
    "### $Â E(\\theta) = \\frac{1}{n}\\times\\displaystyle\\sum_{i=1} ^n L(h_\\theta(x_i) - y_i)$\n",
    "\n",
    "### $L(h_\\theta(x), y) = \\begin{cases}\n",
    "      -log_2(h_\\theta(x)) \\;\\;if y=1\\\\\n",
    "      -log_2(1 - h_\\theta(x)) \\;\\;if y=0\\\\\n",
    "    \\end{cases} $\n",
    "    \n",
    "To explain this in a simple way, when the true value $y=1$ and $h_\\theta(x)$ is closer to $1$, the cost will be closer to $0$, i.e lower cost and vice versa. This goes in the opposite direction when the true ouput i $y=0$ and $h_\\theta(x)$ is closer to $0$, the cost is closer to $0$ as well, which is dedicated by the piecewise functions if statements. \n",
    "\n",
    "\n",
    "#### Multi-class classification\n",
    "As the cost function suggests Logistic Regression only works for binary classification, that's not true. We can use it for multi-class outputs as well by a simple idea. If we treat the every instance as \"one-vs-all\", we can predict a probability for every possible class and choose the one that's highest. **There are much more details into this and I will update this section later**.\n",
    "\n",
    "\n",
    "\n",
    "### K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b2f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code example\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db955a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
